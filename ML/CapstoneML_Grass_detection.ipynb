{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install piexif"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHYAcB-TGYyr",
        "outputId": "5e71718c-a194-4690-b1c0-0cbb4a1e1596"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting piexif\n",
            "  Downloading piexif-1.1.3-py2.py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading piexif-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: piexif\n",
            "Successfully installed piexif-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL8y7btHGliN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import piexif\n",
        "from PIL import Image\n",
        "from PIL.ExifTags import TAGS, GPSTAGS\n",
        "import json\n",
        "import random\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-2cDSbrIAWE",
        "outputId": "8f3f8788-c5af-438b-fb25-94813ebf80fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2RdqziNUyMN"
      },
      "outputs": [],
      "source": [
        "train_dir = '/content/drive/Shared drives/vertani-datasets/GrassDetectionDataset/train'\n",
        "validation_dir = '/content/drive/Shared drives/vertani-datasets/GrassDetectionDataset/validation'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0_RFrq7vmwq"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    validation_split=0.2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTB82Aqexs29",
        "outputId": "3dba2043-a1da-4942-eb39-bb18ae8dfee3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 244 images belonging to 2 classes.\n",
            "Found 12 images belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwQLemf-xv6U"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Input(shape=(150, 150, 3)),\n",
        "    Conv2D(32, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(2, 2),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G3bA4swx3Rr",
        "outputId": "730e390c-8963-43cf-aaab-9260349e4337"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 3s/step - accuracy: 0.5320 - loss: 1.5666 - val_accuracy: 0.5000 - val_loss: 0.6954\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.6223 - loss: 0.6098 - val_accuracy: 0.9167 - val_loss: 0.3766\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 167ms/step - accuracy: 0.7991 - loss: 0.4427 - val_accuracy: 0.8333 - val_loss: 0.2686\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 165ms/step - accuracy: 0.8337 - loss: 0.3773 - val_accuracy: 0.9167 - val_loss: 0.1794\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.8384 - loss: 0.3533 - val_accuracy: 0.9167 - val_loss: 0.1795\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 211ms/step - accuracy: 0.8585 - loss: 0.2930 - val_accuracy: 0.9167 - val_loss: 0.1258\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.8228 - loss: 0.4199 - val_accuracy: 0.9167 - val_loss: 0.1424\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.8942 - loss: 0.2897 - val_accuracy: 0.9167 - val_loss: 0.1160\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 160ms/step - accuracy: 0.9018 - loss: 0.2883 - val_accuracy: 1.0000 - val_loss: 0.0740\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 164ms/step - accuracy: 0.9298 - loss: 0.1865 - val_accuracy: 1.0000 - val_loss: 0.0457\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gi9jbneax4QY",
        "outputId": "75348784-95aa-4654-a1c9-ed811cb0edb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 914ms/step - accuracy: 1.0000 - loss: 0.0457\n",
            "Validation accuracy: 1.00\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(validation_generator)\n",
        "print(f'Validation accuracy: {accuracy:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hGHiZiRLlZ4Y",
        "outputId": "158c7acf-4750-473d-fe0f-9f352f1047f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ],
      "source": [
        "model.save('/content/drive/Shared drives/vertani-datasets/GrassDetectionDataset/grass_detection_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_json = model.to_json()\n",
        "with open('/content/drive/Shared drives/vertani-datasets/grass_detection_model.json', 'w') as file:\n",
        "    file.write(model_json)\n",
        "\n",
        "model.save_weights('/content/drive/Shared drives/vertani-datasets/grass_detection_model.weights.h5')"
      ],
      "metadata": {
        "id": "8swDa5wNGliX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h5_model_path = '/content/drive/Shared drives/vertani-datasets/GrassDetectionDataset/grass_detection_model.h5'\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model(h5_model_path))\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_model_path = '/content/drive/Shared drives/vertani-datasets/grass_detection_model.tflite'\n",
        "with open(tflite_model_path, 'wb') as file:\n",
        "    file.write(tflite_model)\n",
        "\n",
        "print(f\"TFLite model saved at {tflite_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raMNUljtAvg0",
        "outputId": "831f250c-1246-4fbb-eb19-bcbe19b501f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpw3n0lo1i'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 150, 150, 3), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  132206409836752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206409841680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206409869872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206409873392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206410015392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206410013456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206410020144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206410190864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206410191216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  132206410190160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "TFLite model saved at /content/drive/Shared drives/vertani-datasets/grass_detection_model.tflite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install tflite-support\n"
      ],
      "metadata": {
        "id": "Jz-YNDJ3G1A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from tflite_support import metadata as _metadata\n",
        "# from tflite_support.metadata_writers import image_classifier\n",
        "# # Path to the TFLite model file and labels file\n",
        "# labels_file_path = '/content/drive/Shared drives/vertani-datasets/labels.txt'\n",
        "\n",
        "# # Create metadata for the TFLite model\n",
        "# metadata_writer = image_classifier.MetadataWriter.create_for_inference(\n",
        "#     tflite_model_path=tflite_model_path,\n",
        "#     label_file_path=labels_file_path\n",
        "# )\n",
        "\n",
        "# # Save the model with metadata\n",
        "# metadata_writer.write_metadata('/content/drive/Shared drives/vertani-datasets/grass_detection_model_with_metadata.tflite')\n",
        "# print(\"TFLite model with metadata saved!\")\n"
      ],
      "metadata": {
        "id": "g-clsQ7PG4-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# h5_model_path = '/content/drive/Shared drives/vertani-datasets/GrassDetectionDataset/grass_detection_model.h5'\n",
        "\n",
        "# tflite_model_path = '/content/drive/Shared drives/vertani-datasets/grass_detection_model.tflite'\n",
        "\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(tf.keras.models.load_model(h5_model_path))\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# with open(tflite_model_path, 'wb') as file:\n",
        "#     file.write(tflite_model)"
      ],
      "metadata": {
        "id": "DOmziH3tGnEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4Z1sPnPnGSY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6aa9a4f7-072a-4bdc-b878-7afc459c09a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('/content/drive/Shared drives/vertani-datasets/GrassDetectionDataset/grass_detection_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLBsMScXnN0L"
      },
      "outputs": [],
      "source": [
        "img_path = '/content/drive/Shared drives/vertani-datasets/test/Pmetadata.jpg'\n",
        "img = load_img(img_path, target_size=(150, 150))\n",
        "img_array = img_to_array(img) / 255.0\n",
        "img_array = np.expand_dims(img_array, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4HzHKzZnVhi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d87608-e9f7-4c48-bc80-54e494209e8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "[[0.99998105]]\n",
            "Grass Detected!\n"
          ]
        }
      ],
      "source": [
        "prediction = model.predict(img_array)\n",
        "print(prediction)\n",
        "if prediction > 0.5:\n",
        "    print(\"Grass Detected!\")\n",
        "else:\n",
        "    print(\"No Grass Detected!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4jPiZsTnYTD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "1119bff0-7940-4936-e244-2f9d1d8bba55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of grass area: 0.12%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=375x500>"
            ],
            "image/png": "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\n",
            "image/jpeg": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "original_img = cv2.imread(img_path)\n",
        "\n",
        "hsv_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "lower_bound = np.array([35, 50, 50])\n",
        "upper_bound = np.array([85, 255, 255])\n",
        "\n",
        "grass_mask = cv2.inRange(hsv_img, lower_bound, upper_bound)\n",
        "grass_area = cv2.bitwise_and(original_img, original_img, mask=grass_mask)\n",
        "\n",
        "grass_pixel_count = cv2.countNonZero(grass_mask)\n",
        "total_pixel_count = grass_mask.size\n",
        "grass_percentage = round((grass_pixel_count / total_pixel_count) * 100, 2)\n",
        "print(f\"Percentage of grass area: {grass_percentage:.2f}%\")\n",
        "\n",
        "max_width = 500\n",
        "max_height = 500\n",
        "\n",
        "height, width = original_img.shape[:2]\n",
        "\n",
        "if width > max_width or height > max_height:\n",
        "    aspect_ratio = width / height\n",
        "    if aspect_ratio > 1:\n",
        "        new_width = max_width\n",
        "        new_height = int(new_width / aspect_ratio)\n",
        "    else:\n",
        "        new_height = max_height\n",
        "        new_width = int(new_height * aspect_ratio)\n",
        "    resized_img = cv2.resize(grass_area, (new_width, new_height))\n",
        "else:\n",
        "    resized_img = grass_area\n",
        "\n",
        "cv2_imshow(resized_img)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_lat_long(img_path):\n",
        "    try:\n",
        "        image = Image.open(img_path)\n",
        "        exif_data = image._getexif()\n",
        "\n",
        "        if exif_data is not None:\n",
        "            gps_info = None\n",
        "            for tag, value in exif_data.items():\n",
        "                if TAGS.get(tag) == 'GPSInfo':\n",
        "                    gps_info = value\n",
        "                    break\n",
        "\n",
        "            if gps_info is not None:\n",
        "                gps_data = {}\n",
        "                for key in gps_info:\n",
        "                    if key in GPSTAGS:\n",
        "                        gps_data[GPSTAGS[key]] = gps_info[key]\n",
        "\n",
        "                latitude = None\n",
        "                longitude = None\n",
        "\n",
        "                if 'GPSLatitude' in gps_data and 'GPSLatitudeRef' in gps_data:\n",
        "                    lat = gps_data['GPSLatitude']\n",
        "                    lat_ref = gps_data['GPSLatitudeRef']\n",
        "                    latitude = convert_to_degrees(lat)\n",
        "                    if lat_ref != 'N':\n",
        "                        latitude = -latitude\n",
        "\n",
        "                if 'GPSLongitude' in gps_data and 'GPSLongitudeRef' in gps_data:\n",
        "                    lon = gps_data['GPSLongitude']\n",
        "                    lon_ref = gps_data['GPSLongitudeRef']\n",
        "                    longitude = convert_to_degrees(lon)\n",
        "                    if lon_ref != 'E':\n",
        "                        longitude = -longitude\n",
        "\n",
        "                return latitude, longitude\n",
        "            else:\n",
        "                print(\"GPSInfo not found in the metadata.\")\n",
        "        else:\n",
        "            print(\"No EXIF data found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    return None, None\n",
        "\n",
        "def convert_to_degrees(value):\n",
        "    \"\"\"Convert the GPS coordinates to degrees.\"\"\"\n",
        "    d = value[0]\n",
        "    m = value[1]\n",
        "    s = value[2]\n",
        "    return d + (m / 60.0) + (s / 3600.0)\n",
        "\n",
        "latitude, longitude = get_lat_long(img_path)\n",
        "if latitude is not None and longitude is not None:\n",
        "    print(f'Latitude: {latitude}, Longitude: {longitude}')\n",
        "else:\n",
        "    print('Latitude and Longitude data not available')"
      ],
      "metadata": {
        "id": "FrIjD3_LHJQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24fc1fe7-bd76-4397-afef-a953196bb200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latitude: -7.023602777777778, Longitude: 107.54797777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "def get_elevation(lat, lon):\n",
        "    url = f\"https://api.open-meteo.com/v1/elevation?latitude={lat}&longitude={lon}\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        elevation = data[\"elevation\"][0]\n",
        "        return elevation\n",
        "    except requests.exceptions.RequestException as req_err:\n",
        "        print(f\"HTTP Request error: {req_err}\")\n",
        "    except KeyError:\n",
        "        print(\"Error parsing response: 'elevation' key not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred: {e}\")\n",
        "    return None\n",
        "\n",
        "elevation = get_elevation(latitude, longitude)\n",
        "if elevation is not None:\n",
        "    print(f\"Elevasi lokasi Anda: {elevation} meter\")\n",
        "else:\n",
        "    print(\"Gagal mendapatkan elevasi lokasi.\")\n"
      ],
      "metadata": {
        "id": "5wLnsMFcHKwl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62377ee1-5b88-470b-8459-6d173abf369c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elevasi lokasi Anda: 671.0 meter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/Shared drives/vertani-datasets/PlantRecomendationDatasets/new_vegetables_v2.csv'\n",
        "image_folder = '/content/drive/Shared drives/vertani-datasets/PlantRecomendationDatasets/Plant-images'"
      ],
      "metadata": {
        "id": "IemGibKzHNLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_plants(grass_percentage, elevation, dataset_path, image_folder):\n",
        "    data = pd.read_csv(dataset_path)\n",
        "\n",
        "    if grass_percentage < 5:\n",
        "        return \"Error: The plantable area is not enough. Please retake the picture.\"\n",
        "\n",
        "    if 5 <= grass_percentage < 20:\n",
        "        filtered_plants = data[data['max_ukuran'] < 1]\n",
        "    elif 20 <= grass_percentage < 50:\n",
        "        filtered_plants = data[data['max_ukuran'] < 2]\n",
        "    else:\n",
        "        filtered_plants = data\n",
        "\n",
        "    filtered_plants = filtered_plants[\n",
        "        (filtered_plants['min_mdpl'] <= elevation) &\n",
        "        (filtered_plants['max_mdpl'] >= elevation)\n",
        "    ]\n",
        "\n",
        "    if filtered_plants.empty:\n",
        "        return \"Error: Currently, there is no plant that fits your data. Please ask the app developer to improve the datasets.\"\n",
        "\n",
        "    if len(filtered_plants) > 4:\n",
        "        filtered_plants = filtered_plants.sample(n=4, random_state=random.randint(0, 1000))\n",
        "\n",
        "    print(f\"Based on the photo you uploaded, you have a plantable area of {grass_percentage}% and an altitude of {elevation} MDPL.\\n\")\n",
        "    print(\"Here are our plant recommendations that can fit your circumstances:\\n\")\n",
        "\n",
        "    for _, row in filtered_plants.iterrows():\n",
        "        plant_name = row['nama_sayuran']\n",
        "        description = row['penjelasan']\n",
        "        image_path = os.path.join(image_folder, f\"{plant_name}.jpg\")\n",
        "\n",
        "        print(f\"- {plant_name}\")\n",
        "        if os.path.exists(image_path):\n",
        "            img = plt.imread(image_path)\n",
        "            plt.imshow(img)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"(Image not available)\")\n",
        "        print(f\"{description}\\n\")\n",
        "\n",
        "recommend_plants(grass_percentage, elevation, dataset_path, image_folder)"
      ],
      "metadata": {
        "id": "efsKFBtBHPks",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "04f8d173-7405-4a52-8d10-fc6199888491"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Error: The plantable area is not enough. Please retake the picture.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}